        <!-- Solution: MGS -->
        <section id="methodology" class="bg-indigo-600 rounded-[3rem] p-12 text-white relative overflow-hidden">
            <div class="absolute top-0 right-0 w-1/2 h-full bg-gradient-to-l from-white/10 to-transparent pointer-events-none"></div>
            
            <div class="relative z-10 grid md:grid-cols-2 gap-12 items-center">
                <div class="space-y-8">
                    <div class="inline-block px-3 py-1 bg-white/20 rounded-full text-xs font-bold tracking-widest uppercase">The Solution</div>
                    <h2 class="text-4xl md:text-5xl font-black leading-tight">Modular <br>Gradient Surgery</h2>
                    <p class="text-indigo-100 text-lg leading-relaxed">
                        Instead of treating the model as a monolithic block, MGS partitions parameters into <b>functional modules</b> (MLPs, Attention) and applies Gradient Surgery (PCGrad) independently.
                    </p>
                    <ul class="space-y-4">
                        <li class="flex items-center gap-3">
                            <i data-lucide="check-circle" class="text-indigo-300"></i>
                            <span><b>Local Resolution:</b> Prevents one module's conflict from stalling the whole model.</span>
                        </li>
                        <li class="flex items-center gap-3">
                            <i data-lucide="check-circle" class="text-indigo-300"></i>
                            <span><b>Functional Specialization:</b> Preserves domain-specific knowledge in MLPs.</span>
                        </li>
                        <li class="flex items-center gap-3">
                            <i data-lucide="check-circle" class="text-indigo-300"></i>
                            <span><b>FSDP Compatible:</b> Overhead is negligible (~0.1% training time).</span>
                        </li>
                    </ul>
                </div>

                <div class="bg-slate-900 rounded-3xl p-6 shadow-2xl font-mono text-[11px] border border-white/10">
                    <div class="flex justify-between items-center mb-4 border-b border-slate-800 pb-2">
                        <span class="text-slate-500 uppercase font-bold tracking-tighter">mgs_algorithm.py</span>
                        <div class="flex gap-1.5">
                            <div class="w-2.5 h-2.5 rounded-full bg-red-500"></div>
                            <div class="w-2.5 h-2.5 rounded-full bg-yellow-500"></div>
                            <div class="w-2.5 h-2.5 rounded-full bg-green-500"></div>
                        </div>
                    </div>
                    <div class="space-y-1">
                        <p><span class="text-purple-400">def</span> <span class="text-blue-400">apply_mgs</span>(gradients, modules):</p>
                        <p>&nbsp;&nbsp;<span class="text-purple-400">for</span> task_a, task_b <span class="text-purple-400">in</span> permutations(tasks, 2):</p>
                        <p>&nbsp;&nbsp;&nbsp;&nbsp;<span class="text-slate-500"># Iterate per physical module layer</span></p>
                        <p>&nbsp;&nbsp;&nbsp;&nbsp;<span class="text-purple-400">for</span> mod <span class="text-purple-400">in</span> modules:</p>
                        <p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;g_a = gradients[task_a][mod]</p>
                        <p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;g_b = gradients[task_b][mod]</p>
                        <p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="text-purple-400">if</span> dot(g_a, g_b) < 0:</p>
                        <p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;project(g_a, onto=g_b_orthogonal_plane)</p>
                    </div>
                </div>
            </div>
        </section>


                <!-- The Problem: Challenges of Reasoning Post-Training -->
        <section id="challenges" class="space-y-16">
            <div class="text-center max-w-3xl mx-auto space-y-4">
                <div class="section-tag">Empirical Analysis</div>
                <h2 class="text-4xl font-extrabold text-slate-900">Why does Multi-task Reasoning fail?</h2>
                <p class="text-slate-500 text-lg font-medium">Current strategies suffer from interference at both the behavioral and optimization levels.</p>
            </div>

            <div class="grid md:grid-cols-2 gap-8">
                <!-- Column 1: Sequential RL & Behavioral interference -->
                <div class="space-y-8">
                    <div class="p-8 bg-white border border-slate-200 rounded-[2rem] shadow-sm space-y-6">
                        <div class="flex items-center gap-4">
                            <div class="w-12 h-12 bg-red-100 text-red-600 rounded-2xl flex items-center justify-center">
                                <i data-lucide="alert-triangle"></i>
                            </div>
                            <h3 class="text-xl font-bold">The Sequential RL Trap</h3>
                        </div>
                        <p class="text-slate-600 text-sm leading-relaxed">
                            Training domains sequentially leads to <b>Mode Interference</b>. If we train Math after Chat, Chat performance collapses (Forgetting). If we train Chat after Math, the model's low-entropy state prevents effective exploration (Rigidity).
                        </p>
                        <div class="bg-red-50 p-4 rounded-xl space-y-3">
                            <div class="flex justify-between items-center text-xs font-bold text-red-800">
                                <span>Forgetting (Chat Score Drop)</span>
                                <span>-7.0 pts</span>
                            </div>
                            <div class="flex justify-between items-center text-xs font-bold text-red-800">
                                <span>Rigidity Gap (Math &rarr; Chat)</span>
                                <span>-9.6 pts</span>
                            </div>
                        </div>
                        <img src="assets/rigidity_forgetting_v2.svg" alt="Mode Interference" class="w-full rounded-xl">
                    </div>
                </div>

                <!-- Column 2: Gradient Conflicts -->
                <div class="space-y-8">
                    <div class="p-8 bg-slate-900 text-white rounded-[2rem] shadow-xl space-y-6">
                        <div class="flex items-center gap-4">
                            <div class="w-12 h-12 bg-blue-500/20 text-blue-400 rounded-2xl flex items-center justify-center">
                                <i data-lucide="git-merge"></i>
                            </div>
                            <h3 class="text-xl font-bold">The Gradient Paradox</h3>
                        </div>
                        <p class="text-slate-300 text-sm leading-relaxed">
                            Mixed data training causes conflicts where task gradients point in opposite directions. Even a 90% Math mixture fails to match a Math-only model because of these <b>Negative Cosine Similarities</b>.
                        </p>
                        <img src="assets/stacked_ratio_cos_vertical_log.svg" alt="Gradient Conflicts" class="w-full rounded-xl border border-slate-700">
                        <p class="text-[10px] text-slate-500 font-mono">Figure: High frequency of negative alignment between Chat and Math gradients during training.</p>
                    </div>
                </div>
            </div>
            
            <div class="figure-card max-w-4xl mx-auto">
                <img src="assets/entropy_analysis_vertical.svg" alt="Entropy Tradeoff" class="figure-img" onerror="this.src='https://placehold.co/1000x400?text=Entropy+And+Capability+Analysis'">
                <p class="text-xs text-slate-500 text-center"><b>Entropy vs. Performance:</b> Sequential methods lead to irreversible entropy collapse in reasoning trajectories.</p>
            </div>
        </section>